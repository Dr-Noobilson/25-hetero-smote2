{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "# from train_data import input_data\n",
    "# import argparse\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../aminer/\"\n",
    "neigh_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neighbour reader for all nodes\n",
    "# data_path = \"../aminer/\"\n",
    "# neigh_dict = defaultdict(list)\n",
    "# file_names = ['a_p_train', 'p_a_train', 'p_v_train', 'v_p_train']\n",
    "\n",
    "# for k in range(len(file_names)):\n",
    "    \n",
    "#     with open(data_path + file_names[k] + '.txt', 'r') as file:            \n",
    "#         lines = file.readlines()\n",
    "    \n",
    "#     for i, line in enumerate(lines):\n",
    "#         node_id, neigh = line.strip().split(':')\n",
    "#         neigh_list = neigh.split(',')\n",
    "#         # print(node_id, neigh_list)\n",
    "        \n",
    "#         if k == 0:\n",
    "#             for node in neigh_list:\n",
    "#                 neigh_dict[f'a{node_id}'].append(f'p{node}')\n",
    "#         elif k == 1:\n",
    "#             for node in neigh_list:\n",
    "#                 neigh_dict[f'p{node_id}'].append(f'a{node}')\n",
    "#         elif k == 2:\n",
    "#             for node in neigh_list:\n",
    "#                 neigh_dict[f'p{node_id}'].append(f'v{node}')\n",
    "#         else: \n",
    "#             for node in neigh_list:\n",
    "#                 if int(node) < 13250:\n",
    "#                     neigh_dict[f'v{node_id}'].append(f'p{node}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a14', 'a15', 'a16', 'a17', 'a18', 'a19', 'a20', 'a21', 'a22', 'a23', 'a24', 'a25', 'a26', 'a27', 'a28', 'a29', 'a30', 'a31', 'a32', 'a33', 'a34', 'a35', 'a36', 'a37', 'a38', 'a39', 'a40', 'a41', 'a42', 'a43', 'a44', 'a45', 'a46', 'a47', 'a48', 'a49', 'a50', 'a51', 'a52', 'a53', 'a54', 'a55', 'a56', 'a57', 'a58', 'a59', 'a60', 'a61', 'a62', 'a63', 'a64', 'a65', 'a66', 'a67', 'a68', 'a69', 'a70', 'a71', 'a72', 'a73', 'a74', 'a75', 'a76', 'a77', 'a78', 'a79', 'a80', 'a81', 'a82', 'a83', 'a84', 'a85', 'a86', 'a87', 'a88', 'a89', 'a90', 'a91', 'a92', 'a93', 'a94', 'a95', 'a96', 'a97', 'a98', 'a99']\n"
     ]
    }
   ],
   "source": [
    "# print(list(neigh_dict.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_walks = defaultdict(list)\n",
    "\n",
    "# for node in list(neigh_dict.keys()):\n",
    "#     print(node)\n",
    "#     curNode = node\n",
    "    \n",
    "#     walk_size, a_size, p_size, v_size = 0,0,0,0\n",
    "    \n",
    "#     while walk_size < 100:\n",
    "#         prob =  random.random()\n",
    "#         if prob < 0.5:\n",
    "#             curNode = node\n",
    "#         else:\n",
    "#             if curNode not in list(neigh_dict.keys()):\n",
    "#                 print(node, curNode)\n",
    "#             curNode = random.choice(neigh_dict[curNode])\n",
    "#             if curNode != node:\n",
    "#                 if curNode[0] == 'a' and a_size < 46:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     a_size += 1\n",
    "#                 elif curNode[0] == 'p' and p_size < 46:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     p_size += 1\n",
    "#                 elif curNode[0] == 'v' and v_size < 11:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     v_size += 1\n",
    "                \n",
    "#                 walk_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13', 'a14', 'a15', 'a16', 'a17', 'a18', 'a19', 'a20', 'a21', 'a22', 'a23', 'a24', 'a25', 'a26', 'a27', 'a28', 'a29', 'a30', 'a31', 'a32', 'a33', 'a34', 'a35', 'a36', 'a37', 'a38', 'a39', 'a40', 'a41', 'a42', 'a43', 'a44', 'a45', 'a46', 'a47', 'a48', 'a49', 'a50', 'a51', 'a52', 'a53', 'a54', 'a55', 'a56', 'a57', 'a58', 'a59', 'a60', 'a61', 'a62', 'a63', 'a64', 'a65', 'a66', 'a67', 'a68', 'a69', 'a70', 'a71', 'a72', 'a73', 'a74', 'a75', 'a76', 'a77', 'a78', 'a79', 'a80', 'a81', 'a82', 'a83', 'a84', 'a85', 'a86', 'a87', 'a88', 'a89', 'a90', 'a91', 'a92', 'a93', 'a94', 'a95', 'a96', 'a97', 'a98', 'a99']\n"
     ]
    }
   ],
   "source": [
    "# print(list(random_walks.keys())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('het_neigh_train.txt', 'w') as file:\n",
    "#     for node in list(random_walks.keys()):\n",
    "#         file.write(node + ':' + ','.join(random_walks[node])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node net embedding generator\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import *\n",
    "dimen = 128\n",
    "window = 5\n",
    "\n",
    "def read_random_walk_corpus():\n",
    "    walks, node_ids = [], []\n",
    "    with open(\"het_neigh_train.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.strip().split(':')\n",
    "            node_ids.append(parts[0])\n",
    "            path = parts[1].split(',')\n",
    "            walks.append(path)\n",
    "\n",
    "    return walks, node_ids\n",
    "\n",
    "walk_corpus, node_ids = read_random_walk_corpus()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(walk_corpus, vector_size=dimen, window=window, min_count=0, workers=8, sg=1, hs=0, negative=5)\n",
    "\n",
    "# Save node embeddings to a text file\n",
    "with open(\"node_net_embedding.txt\", 'w') as file:\n",
    "    for i, node_id in enumerate(node_ids):\n",
    "        embedding = model.wv[node_id]\n",
    "        embedding_str = \" \".join(str(val) for val in embedding)\n",
    "        file.write(node_id + \" \" + embedding_str + \"\\n\")\n",
    "model.wv.save_word2vec_format(\"node_net_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"p_p_train.txt\", 'r') as file1:            \n",
    "#     lines1 = file1.readlines()\n",
    "    \n",
    "# with open(data_path + \"p_p_edge_pred.txt\", 'w') as file2:            \n",
    "\n",
    "#     for i, line in enumerate(lines1):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         node_neigh= re.split(',', re.split(':', line)[1])\n",
    "    \n",
    "#         for neigh in node_neigh:\n",
    "#             file2.write(node_id+ ',' + neigh + \",1\\n\")\n",
    "        \n",
    "#         for i in range(len(node_neigh)):\n",
    "#             while True:\n",
    "#                 rand = str(np.random.randint(0, 13249))\n",
    "#                 if rand not in node_neigh: break \n",
    "#             file2.write(node_id+ ',' + rand + \",0\\n\")\n",
    "\n",
    "# for i, line in enumerate(lines2):\n",
    "#     line = line.strip()\n",
    "#     node_id = re.split(',', line)[0].strip()\n",
    "#     id2.append(node_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is list2 a subset of list1? True\n",
      "Elements from list2 not in list1: []\n"
     ]
    }
   ],
   "source": [
    "# # Check if list2 is a subset of list1\n",
    "# is_subset = set(id2).issubset(set(id1))\n",
    "\n",
    "# # Find elements from list2 that are not in list1\n",
    "# elements_not_in_list1 = [item for item in id2 if item not in id1]\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Is list2 a subset of list1? {is_subset}\")\n",
    "# print(f\"Elements from list2 not in list1: {elements_not_in_list1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(data_path + \"het_neigh_train.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()\n",
    "\n",
    "# with open(\"het_neigh_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "#         node_type = node_id[0]\n",
    "#         if i == 0: print(len(neigh_list))\n",
    "        \n",
    "#         if node_type == 'v':\n",
    "#             file5.write(str(node_id)+':')\n",
    "#         elif node_type == 'a':\n",
    "#             try:\n",
    "#                 file5.write('a' + str(dict_a[node_id[1:]]) + ':')\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#         else:\n",
    "#             try:\n",
    "#                 file5.write('p' + str(dict_p[node_id[1:]]) + ':')\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "                   \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             node_t = neigh_list[j][0]\n",
    "#             if node_t == 'v':\n",
    "#                 file5.write(str(neigh_list[j]))\n",
    "#             elif node_t == 'a':\n",
    "#                 try:\n",
    "#                     file5.write('a' + str(dict_a[neigh_list[j][1:]]))\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     file5.write('p' + str(dict_p[neigh_list[j][1:]]))\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "                \n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"a_p_train.txt\", 'w') as file3:            \n",
    "#     for i, line in enumerate(lines1):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "#         file3.write(str(dict_a[node_id]) + ':')\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file3.write(str(dict_p[neigh_list[j]]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file3.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file3.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"p_a_train.txt\", 'w') as file4:            \n",
    "#     for i, line in enumerate(lines2):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "#         file4.write(str(dict_p[node_id]) + ':')\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file4.write(str(dict_a[neigh_list[j]]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file4.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file4.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"p_p_cite_list_train.txt\", 'r') as filex:            \n",
    "#     lines3 = filex.readlines()\n",
    "    \n",
    "# with open(\"p_p_train.txt\", 'w') as file5:            \n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "#         file5.write(str(dict_p[node_id]) + ':')\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(dict_p[neigh_list[j]]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"v_p_list_train.txt\", 'r') as filex:            \n",
    "#     lines3 = filex.readlines()\n",
    "    \n",
    "# with open(\"v_p_train.txt\", 'w') as file5:            \n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "#         file5.write(str(node_id) + ':')\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(dict_p[neigh_list[j]]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"p_v_list.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()\n",
    "\n",
    "# with open(\"p_v_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(':', line)[0]\n",
    "#         neigh_list = re.split(',', re.split(':', line)[1])\n",
    "        \n",
    "#         try:\n",
    "#             file5.write(str(dict_p[node_id]) + ':')\n",
    "#         except KeyError:\n",
    "#             # Handle the case where node_id is not found in dict_p\n",
    "#             continue\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(neigh_list[j]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(',')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# with open(data_path + \"p_title_embed.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()[1:]\n",
    "\n",
    "# with open(\"p_title_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(' ', line)[0]\n",
    "#         neigh_list = re.split(' ', line)[1:]\n",
    "#         if i == 0: print(len(neigh_list))\n",
    "        \n",
    "#         try:\n",
    "#             file5.write(str(dict_p[node_id]) + ' ')\n",
    "#         except KeyError:\n",
    "#             # Handle the case where node_id is not found in dict_p\n",
    "#             continue\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(neigh_list[j]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(' ')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# with open(data_path + \"p_abstract_embed.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()[1:]\n",
    "\n",
    "# with open(\"p_abstract_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(' ', line)[0]\n",
    "#         neigh_list = re.split(' ', line)[1:]\n",
    "#         if i == 0: print(len(neigh_list))\n",
    "        \n",
    "#         try:\n",
    "#             file5.write(str(dict_p[node_id]) + ' ')\n",
    "#         except KeyError:\n",
    "#             # Handle the case where node_id is not found in dict_p\n",
    "#             continue\n",
    "                \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(neigh_list[j]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(' ')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"node_net_embedding.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()[2:]\n",
    "\n",
    "# with open(\"node_net_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(' ', line)[0]\n",
    "#         neigh_list = re.split(' ', line)[1:]\n",
    "#         node_type = node_id[0]\n",
    "        \n",
    "#         if node_type == 'v':\n",
    "#             file5.write(str(node_id)+' ')\n",
    "#         elif node_type == 'a':\n",
    "#             try:\n",
    "#                 file5.write('a' + str(dict_a[node_id[1:]]) + ' ')\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#         else:\n",
    "#             try:\n",
    "#                 file5.write('p' + str(dict_p[node_id[1:]]) + ' ')\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "                   \n",
    "#         for j in range(len(neigh_list)):\n",
    "#             file5.write(str(neigh_list[j]))\n",
    "#             if j != len(neigh_list) - 1:\n",
    "#                 file5.write(' ')\n",
    "        \n",
    "#         # Move to the next line\n",
    "#         file5.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"a_class_train.txt\", 'r') as filex:\n",
    "#     lines3 = filex.readlines()\n",
    "\n",
    "# with open(\"a_class_train.txt\", 'w') as file5:\n",
    "#     for i, line in enumerate(lines3):\n",
    "#         line = line.strip()\n",
    "#         node_id = re.split(',', line)\n",
    "        \n",
    "#         try:\n",
    "#             file5.write(str(dict_a[node_id[0]]) + ',' + str(node_id[1]))\n",
    "#         except KeyError:\n",
    "#             continue\n",
    "                \n",
    "#         file5.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
