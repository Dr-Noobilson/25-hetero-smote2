{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import torch\n",
    "from collections import Counter\n",
    "# from torch_geometric.datasets import DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = {}\n",
    "d_dict = {}\n",
    "c_dict = {}\n",
    "s_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"node.dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"data2/gene.txt\", 'w') as gfile:\n",
    "    num = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        entries = line.strip().split()\n",
    "        if entries[2] == '0':\n",
    "            g_dict[entries[0]] = num\n",
    "            gfile.write(f'{num} {entries[-1]}\\n')\n",
    "            num += 1\n",
    "    print(num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"node.dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"data2/disease.txt\", 'w') as dfile:\n",
    "    num = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        entries = line.strip().split()\n",
    "        if entries[2] == '1':\n",
    "            d_dict[entries[0]] = num\n",
    "            dfile.write(f'{num} {entries[-1]}\\n')\n",
    "            num += 1\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"node.dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"data2/chemical.txt\", 'w') as cfile:\n",
    "    num = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        entries = line.strip().split()\n",
    "        if entries[2] == '2':\n",
    "            c_dict[entries[0]] = num\n",
    "            cfile.write(f'{num} {entries[-1]}\\n')\n",
    "            num += 1\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"node.dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(\"data2/species.txt\", 'w') as sfile:\n",
    "    num = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        entries = line.strip().split()\n",
    "        if entries[2] == '3':\n",
    "            s_dict[entries[0]] = num\n",
    "            sfile.write(f'{num} {entries[-1]}\\n')\n",
    "            num += 1\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh_dict = defaultdict(list)\n",
    "# with open(\"link2.dat\", 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# num = 0\n",
    "# for i, line in enumerate(lines):\n",
    "#     entries = line.strip().split()\n",
    "#     edge_type = entries[2]\n",
    "#     node1 = entries[0]\n",
    "#     node2 = entries[1]\n",
    "    \n",
    "#     if edge_type == '0':\n",
    "#         neigh_dict[f'g{g_dict[node1]}'].append(f'g{g_dict[node2]}')\n",
    "#     elif edge_type == '1':\n",
    "#         if node1 not in g_dict.keys(): \n",
    "#             print(node1, i)\n",
    "#             continue\n",
    "#         neigh_dict[f'g{g_dict[node1]}'].append(f'd{d_dict[node2]}')   \n",
    "#         neigh_dict[f'd{d_dict[node2]}'].append(f'g{g_dict[node1]}') \n",
    "#     elif edge_type == '2':\n",
    "#         if d_dict[node1] == 246 or d_dict[node1] == 19920: print(node1)\n",
    "#         neigh_dict[f'd{d_dict[node1]}'].append(f'd{d_dict[node2]}')  \n",
    "#         neigh_dict[f'd{d_dict[node2]}'].append(f'd{d_dict[node1]}') \n",
    "#     elif edge_type == '3':\n",
    "#         neigh_dict[f'c{c_dict[node1]}'].append(f'g{g_dict[node2]}')  \n",
    "#         neigh_dict[f'g{g_dict[node2]}'].append(f'c{c_dict[node1]}') \n",
    "#     elif edge_type == '4':\n",
    "#         neigh_dict[f'c{c_dict[node1]}'].append(f'd{d_dict[node2]}')\n",
    "#         neigh_dict[f'd{d_dict[node2]}'].append(f'c{c_dict[node1]}')  \n",
    "#     elif edge_type == '5':\n",
    "#         neigh_dict[f'c{c_dict[node1]}'].append(f'c{c_dict[node2]}')  \n",
    "#         neigh_dict[f'c{c_dict[node2]}'].append(f'c{c_dict[node1]}') \n",
    "#     elif edge_type == '6':\n",
    "#         neigh_dict[f'c{c_dict[node1]}'].append(f's{s_dict[node2]}') \n",
    "#         neigh_dict[f's{s_dict[node2]}'].append(f'c{c_dict[node1]}') \n",
    "#     elif edge_type == '7':\n",
    "#         neigh_dict[f's{s_dict[node1]}'].append(f'g{g_dict[node2]}') \n",
    "#         neigh_dict[f'g{g_dict[node2]}'].append(f's{s_dict[node1]}')\n",
    "#     elif edge_type == '8':\n",
    "#         neigh_dict[f's{s_dict[node1]}'].append(f'd{d_dict[node2]}') \n",
    "#         neigh_dict[f'd{d_dict[node2]}'].append(f's{s_dict[node1]}')\n",
    "#     elif edge_type == '7':\n",
    "#         neigh_dict[f's{s_dict[node1]}'].append(f's{s_dict[node2]}') \n",
    "#         neigh_dict[f's{s_dict[node2]}'].append(f's{s_dict[node1]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(neigh_dict.keys())[-3:])\n",
    "# print(list(neigh_dict.values())[-5000:])\n",
    "# # print(neigh_dict['gghgaetvaw'])\n",
    "# if 'd246' in list(neigh_dict.keys()):\n",
    "#     print(\"ur mom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random path maker\n",
    "# from tqdm import tqdm\n",
    "# random_walks = defaultdict(list)\n",
    "\n",
    "# for node in tqdm(neigh_dict.keys(), desc=\"Processing Nodes\"):\n",
    "#     # print(node)\n",
    "#     curNode = node\n",
    "    \n",
    "#     walk_size, g_size, d_size, c_size, s_size = 0,0,0,0,0\n",
    "    \n",
    "#     while walk_size < 300:\n",
    "#         prob =  random.random()\n",
    "#         if prob < 0.5:\n",
    "#             curNode = node\n",
    "#         else:\n",
    "#             if curNode not in list(neigh_dict.keys()):\n",
    "#                 # print(node, curNode)\n",
    "#                 curNode = node\n",
    "                \n",
    "#             # print(neigh_dict[curNode])\n",
    "#             if neigh_dict[curNode] != []:\n",
    "#                 curNode = random.choice(neigh_dict[curNode])\n",
    "#                 if curNode[0] == 'g' and g_size < 90:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     g_size += 1\n",
    "#                 elif curNode[0] == 'd' and d_size < 90:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     d_size += 1\n",
    "#                 elif curNode[0] == 'c' and c_size < 120:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     c_size += 1\n",
    "#                 elif curNode[0] == 's' and s_size < 10:\n",
    "#                     random_walks[node].append(curNode)\n",
    "#                     s_size += 1\n",
    "#                 walk_size += 1\n",
    "#             else:\n",
    "#                 curNode = node\n",
    "            \n",
    "#             # if walk_size == 157: print(\"poop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(random_walks['s1232']))\n",
    "# print(random_walks['d20002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data2/het_neigh_train.txt', 'w') as file:\n",
    "#     for node in list(random_walks.keys()):\n",
    "#         file.write(node + ':' + ','.join(random_walks[node])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_edge_list = []\n",
    "d_edge_list = []\n",
    "c_edge_list = []\n",
    "s_edge_list = []\n",
    "g_g_edge_index = []\n",
    "g_d_edge_index = []\n",
    "g_c_edge_index = []\n",
    "g_s_edge_index = []\n",
    "d_g_edge_index = []\n",
    "d_d_edge_index = []\n",
    "d_c_edge_index = []\n",
    "d_s_edge_index = []\n",
    "c_g_edge_index = []\n",
    "c_d_edge_index = []\n",
    "c_c_edge_index = []\n",
    "c_s_edge_index = []\n",
    "s_g_edge_index = []\n",
    "s_d_edge_index = []\n",
    "s_c_edge_index = []\n",
    "s_s_edge_index = []\n",
    "\n",
    "with open(\"data2/het_neigh_train.txt\", 'r') as file:            \n",
    "    lines = file.readlines()\n",
    "        \n",
    "for i, line in enumerate(lines):\n",
    "    line = line.strip()\n",
    "    node_type = re.split(':', line)[0][0]\n",
    "    node_id = int(re.split(':', line)[0][1:])\n",
    "    neigh_list = re.split(',', re.split(':', line)[1].strip())\n",
    "    \n",
    "    g_edge_list = [node for node in neigh_list if node.startswith('g')]\n",
    "    d_edge_list = [node for node in neigh_list if node.startswith('d')]\n",
    "    c_edge_list = [node for node in neigh_list if node.startswith('c')]\n",
    "    s_edge_list = [node for node in neigh_list if node.startswith('s')]\n",
    "    \n",
    "    # Count the frequency of elements starting with 'g', 'd', 'c' and 's'\n",
    "    g_counts = Counter(g_edge_list)\n",
    "    d_counts = Counter(d_edge_list)\n",
    "    c_counts = Counter(c_edge_list)\n",
    "    s_counts = Counter(s_edge_list)\n",
    "    # print(g_counts)\n",
    "    \n",
    "    g_edge_list = [node for node, count in g_counts.most_common(8)]\n",
    "    d_edge_list = [node for node, count in d_counts.most_common(8)]\n",
    "    c_edge_list = [node for node, count in c_counts.most_common(10)]\n",
    "    s_edge_list = [node for node, count in s_counts.most_common(3)]\n",
    "    # print(g_edge_list)\n",
    "    \n",
    "    if node_type == 'g':\n",
    "        g_g_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in g_edge_list])\n",
    "        g_d_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in d_edge_list])\n",
    "        g_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "        g_s_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in s_edge_list])\n",
    "    elif node_type == 'd':\n",
    "        d_g_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in g_edge_list])\n",
    "        d_d_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in d_edge_list])\n",
    "        d_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "        d_s_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in s_edge_list])\n",
    "    elif node_type == 'c':\n",
    "        c_g_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in g_edge_list])\n",
    "        c_d_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in d_edge_list])\n",
    "        c_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "        c_s_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in s_edge_list])\n",
    "    else:\n",
    "        s_g_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in g_edge_list])\n",
    "        s_d_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in d_edge_list])\n",
    "        s_c_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in c_edge_list])\n",
    "        s_s_edge_index.extend([torch.tensor([[node_id, int(node[1:])]]) for node in s_edge_list])\n",
    "\n",
    "# Concatenate the list of tensors into a single tensor\n",
    "g_g_edge_index = torch.cat(g_g_edge_index, dim=0).t().contiguous()\n",
    "g_d_edge_index = torch.cat(g_d_edge_index, dim=0).t().contiguous()\n",
    "g_c_edge_index = torch.cat(g_c_edge_index, dim=0).t().contiguous()\n",
    "g_s_edge_index = torch.cat(g_s_edge_index, dim=0).t().contiguous()\n",
    "d_g_edge_index = torch.cat(d_g_edge_index, dim=0).t().contiguous()\n",
    "d_d_edge_index = torch.cat(d_d_edge_index, dim=0).t().contiguous()\n",
    "d_c_edge_index = torch.cat(d_c_edge_index, dim=0).t().contiguous()\n",
    "d_s_edge_index = torch.cat(d_s_edge_index, dim=0).t().contiguous()\n",
    "c_g_edge_index = torch.cat(c_g_edge_index, dim=0).t().contiguous()\n",
    "c_d_edge_index = torch.cat(c_d_edge_index, dim=0).t().contiguous()\n",
    "c_c_edge_index = torch.cat(c_c_edge_index, dim=0).t().contiguous()\n",
    "c_s_edge_index = torch.cat(c_s_edge_index, dim=0).t().contiguous()\n",
    "s_g_edge_index = torch.cat(s_g_edge_index, dim=0).t().contiguous()\n",
    "s_d_edge_index = torch.cat(s_d_edge_index, dim=0).t().contiguous()\n",
    "s_c_edge_index = torch.cat(s_c_edge_index, dim=0).t().contiguous()\n",
    "s_s_edge_index = torch.cat(s_s_edge_index, dim=0).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data2/gene.txt\", 'r') as gfile:\n",
    "    glines = gfile.readlines()\n",
    "with open(\"data2/disease.txt\", 'r') as dfile:\n",
    "    dlines = dfile.readlines()\n",
    "with open(\"data2/chemical.txt\", 'r') as cfile:\n",
    "    clines = cfile.readlines()\n",
    "with open(\"data2/species.txt\", 'r') as sfile:\n",
    "    slines = sfile.readlines()\n",
    "    \n",
    "lines_list = [glines, dlines, clines, slines]\n",
    "node_id = 'g'\n",
    "\n",
    "with open(\"data2/node_embedding.txt\", 'w') as file:\n",
    "    for j, lines in enumerate(lines_list):\n",
    "        \n",
    "        if j == 0: node_id = 'g'\n",
    "        elif j == 1: node_id = 'd'\n",
    "        elif j == 2: node_id = 'c'\n",
    "        else: node_id = 's'\n",
    "            \n",
    "        for i, line in enumerate(lines):\n",
    "            entries = line.strip().split()            \n",
    "            file.write(f'{node_id}{entries[0]} {entries[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_d = 200\n",
    "g_n, d_n, c_n, s_n = 13561, 20163, 26522, 2863\n",
    "g_embed = torch.zeros(g_n, embed_d, dtype=torch.float32)\n",
    "d_embed = torch.zeros(d_n, embed_d, dtype=torch.float32)\n",
    "c_embed = torch.zeros(c_n, embed_d, dtype=torch.float32) \n",
    "s_embed = torch.zeros(s_n, embed_d, dtype=torch.float32) \n",
    "\n",
    "with open(\"data2/node_embedding.txt\", 'r') as file:\n",
    "    lines =  file.readlines()\n",
    "    \n",
    "for i, line in enumerate(lines):\n",
    "    entries = line.strip().split()\n",
    "    node_id = int(entries[0][1:])\n",
    "    node_type = entries[0][0]\n",
    "    embed_list = entries[1].strip().split(',')\n",
    "    \n",
    "    if node_type == 'g':\n",
    "        g_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "    elif node_type == 'd':\n",
    "        d_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "    elif node_type == 'c':\n",
    "        c_embed[node_id] = torch.tensor([float(value) for value in embed_list], dtype=torch.float32)\n",
    "    elif node_type == 's':\n",
    "        s_embed[node_id] =  torch.tensor([float(value) for value in embed_list], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"label.dat\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "d_label = torch.zeros(454, 2, dtype=torch.long)\n",
    "    \n",
    "with open(\"data2/d_label.txt\", 'w') as file:\n",
    "    for i, line in enumerate(lines):\n",
    "        entries = line.strip().split()\n",
    "        \n",
    "        d_label[i][0] = int(d_dict[entries[0]])\n",
    "        d_label[i][1] = int(entries[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "data = HeteroData()\n",
    "\n",
    "data['g'].num_nodes = g_n\n",
    "data['d'].num_nodes = d_n\n",
    "data['c'].num_nodes = c_n\n",
    "data['s'].num_nodes = s_n\n",
    "\n",
    "data['g'].x = g_embed\n",
    "data['d'].x = d_embed\n",
    "data['c'].x = c_embed\n",
    "data['s'].x = s_embed\n",
    "    \n",
    "# data['g'].x = g_data\n",
    "# data['d'].x = d_data\n",
    "# data['c'].x = c_data\n",
    "\n",
    "data['d'].y = d_label\n",
    "\n",
    "data['g', 'walk', 'g'].edge_index = g_g_edge_index\n",
    "data['g', 'walk', 'd'].edge_index = g_d_edge_index\n",
    "data['g', 'walk', 'c'].edge_index = g_c_edge_index\n",
    "data['g', 'walk', 's'].edge_index = g_s_edge_index\n",
    "\n",
    "data['d', 'walk', 'g'].edge_index = d_g_edge_index\n",
    "data['d', 'walk', 'd'].edge_index = d_d_edge_index\n",
    "data['d', 'walk', 'c'].edge_index = d_c_edge_index\n",
    "data['d', 'walk', 's'].edge_index = d_s_edge_index\n",
    "\n",
    "data['c', 'walk', 'g'].edge_index = c_g_edge_index\n",
    "data['c', 'walk', 'd'].edge_index = c_d_edge_index\n",
    "data['c', 'walk', 'c'].edge_index = c_c_edge_index\n",
    "data['c', 'walk', 's'].edge_index = c_s_edge_index\n",
    "\n",
    "data['s', 'walk', 'g'].edge_index = s_g_edge_index\n",
    "data['s', 'walk', 'd'].edge_index = s_d_edge_index\n",
    "data['s', 'walk', 'c'].edge_index = s_c_edge_index\n",
    "data['s', 'walk', 's'].edge_index = s_s_edge_index\n",
    "\n",
    "# data['g','to','d'].edge_index = ad_edges\n",
    "# data['d','to','g'].edge_index = pg_edges\n",
    "# data['d','to','c'].edge_index = pc_edges\n",
    "# data['c','to','d'].edge_index = td_edges\n",
    "# data['d','to','s'].edge_index = ps_edges\n",
    "# data['s','to','d'].edge_index = cd_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, \"pubmed_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.load('pubmed_data.pt')\n",
    "print(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
